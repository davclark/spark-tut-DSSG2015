{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First things first...\n",
    "\n",
    "Is Spark set up in this notebook? And is it configured for awesome?\n",
    "\n",
    "I recommend `--master local[*]`, which sets up one worker for each core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can still use IPython\n",
    "%ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some RDDs the easy way\n",
    "\n",
    "Spark was conceived as an extension to Map-Reduce, which initially targeted the Hadoop FileSystem (HDFS). Spark uses the notions of Resilient Distributed Datasets (RDDs) that remain in memory, and can abstract over a large variety of storage back-ends.\n",
    "\n",
    "Here, we see how we can work with small data (by some definition) locally without resorting to a cluster.\n",
    "\n",
    "We can easily replace these RDDs with, e.g., an HDFS-backed datastore and cluster, and execute the identical logic. The plan is to introduce this next week.\n",
    "\n",
    "*Note - you will need to copy these data files into the repo or adjust the paths*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permits = sc.textFile('data/Building_Permits.csv')\n",
    "violations = sc.textFile('data/Building_Violations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that those functions returned way faster than the time needed to load the files... Spark is [lazy](http://en.wikipedia.org/wiki/Lazy_evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Spark\n",
    "\n",
    "Don't worry about efficiency, that's what the other cores are for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's do functional programming the way Guido likes it\n",
    "\n",
    "# var, = ... unpacks a sequence of length 1. It is similar to:\n",
    "# var = ...[0]\n",
    "# asseret(len(...) == 1)\n",
    "permit_header_str, = permits.take(1)\n",
    "\n",
    "# Here, we pass a generator expression to enumerate within a dict comprehension\n",
    "# This is very similar to how Spark works, and is *very* memory efficient\n",
    "permit_header_locs = {name: loc for loc, name in \n",
    "               enumerate(val.strip() for val in header_str.split(','))}\n",
    "permit_header_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning some real Spark\n",
    "\n",
    "The core of spark is functional programming over RDDs. Most of you have probably heard of map-reduce. `map` and `reduce` are actually two separate functional primitives, and Spark decouples a small set of powerful primitives that are readiliy mapped to parallel computation over RDD \"partitions.\"\n",
    "\n",
    "In general, these functions take other functions as arguments. PySpark lets you use python functions. Arbitrary expressions (over one variable) can be converted to functions using lambda expressions.\n",
    "\n",
    "Let's explore some of these now.\n",
    "\n",
    "### Filter\n",
    "\n",
    "`filter` iterates over each item in the RDD and returns a new RDD limited to the items where the predicate evaluates to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is dumb. In real life, pre-process your data.\n",
    "# Or parse the files before handing them to Spark.\n",
    "# But, it's a nice intro to Spark...\n",
    "permit_lines = permits.filter(lambda x: x != permit_header_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "\n",
    "`map` returns a set of transformed items that result from applying a predicate to each value. Here, each string gets mapped to a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note that the data is still unparsed (which is efficient, actually)\n",
    "# We are now getting into functional programming with Spark...\n",
    "permit_data = permit_lines.map(lambda x: [val.strip() for val in x.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "permit_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First task\n",
    "\n",
    "Let's convert the above process to a reusable function. \n",
    "\n",
    "### Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assemble the functional statements from above into a re-usable function\n",
    "def simple_csv(rdd):\n",
    "    return None, rdd\n",
    "\n",
    "# And apply it to our violation data\n",
    "violation_header_locs, violation_data = simple_csv(violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other approaches to reading CSV\n",
    "\n",
    "@seahboonsiew wrote some [native PySpark](https://github.com/seahboonsiew/pyspark-csv/blob/master/pyspark_csv.py) to do this with more bells and whistles (but note that it is more expensive: parsing everything).\n",
    "\n",
    "You can also feed Pandas DataFrames to Spark SQL DataFrames (and you can use chunking if need be!) [using standard Spark](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.createDataFrame). And similarly, [you can convert back to Pandas](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas).\n",
    "\n",
    "*NB: there is almost 0 chance you will write a better CSV parser than the one in Pandas.* It will also allow you to avoid parsing columns you don't want to parse (so you don't lose the efficiency we talked about above). But, the parser in Pandas wasn't architected to handle distributed parsing. Don't try to be too clever on this point... just solve *your* problem (not a more general one).\n",
    "\n",
    "I would probably use @mrocklin's [Blaze](http://matthewrocklin.com/slides/sfpython-blaze.html#/2/3), but note that this is alpha/beta code. He's eager to help with social science test-cases though!\n",
    "\n",
    "## Other important functional primitives\n",
    "\n",
    "### Flat Map\n",
    "\n",
    "`.flatMap()` will take sequences of 0 or more elements (`None` is simply discarded) and combine them into a single sequence. I won't cover this today\n",
    "\n",
    "### Reduce\n",
    "\n",
    "`.reduce()` is also in there, and will apply an accumulating function across elements using a pair-wise function. There are many trivial reducing functions such as `.count()`, `.mean()`, `.min()`, and so on. Most of these could be easily implemented with `.reduce()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_col = violation_header_locs['LATITUDE']\n",
    "lat_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, = violation_data.take(1)\n",
    "float(x[lat_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_convert(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "violation_lats = violation_data.map(lambda x: x[lat_col]).map(safe_convert)\n",
    "violation_lats.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This should work, but I'm not sure why it doens't?\n",
    "violation_lats.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that operations like the above should be done in one pass for efficiency! You can also create a Python object that extracts and represents the quantities you're interested in (getting behavior somewhat like an ORM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL-like operations\n",
    "\n",
    "PySpark will do SQL-like operations over regular RDDs, but you should check out [Spark SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html) if you want the best solution.\n",
    "\n",
    "Here, we demo `.keyBy()` and `.join()`. Note that we also have other operations like `.groupBy()`. You can map most SQL concepts onto Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MLlib\n",
    "\n",
    "Always refer to [the docs](http://spark.apache.org/docs/latest/mllib-guide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
